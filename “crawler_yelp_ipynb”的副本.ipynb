{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "“crawler_yelp.ipynb”的副本",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eric9901/WebCrawing-of-Yelp/blob/master/%E2%80%9Ccrawler_yelp_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQsTogoYKhbk",
        "colab_type": "text"
      },
      "source": [
        "## Crawler exercise (YELP)\n",
        "#### by Anthony Lo and his team (https://github.com/team-ant/crawler_example)\n",
        "\n",
        "You will be using Python + BeautifulSoup to create a crawler that perform web scrapping on https://www.yelp.com. Through this exercise, you will know\n",
        "- Send a http request from Python and get server's repsonse\n",
        "- Preprocess scrapped data using BeautifulSoup\n",
        "- Automate the crawler by simple Python coding\n",
        "- Randomize user agent and set timer to minmize the risk of being blocked\n",
        "- Export the data to a file for further analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6puhC00MJC_",
        "colab_type": "text"
      },
      "source": [
        "### Get all shop name in one page\n",
        "Sample output:\n",
        "1. Sugar\n",
        "2. Camper’s\n",
        "3. Mr & Mrs Fox\n",
        "4. Shugetsu\n",
        "5. 金峰靚靚粥麵\n",
        "6. Public\n",
        "7. Plat du Jour\n",
        "8. Formosa Autumn\n",
        "9. Nha Trang\n",
        "10. Wah Cheong Congee & Noodle Shop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2XFy2gIm7TS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCB7nb9Jkd3F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f96d3434-3e94-4e76-b7a3-16b4a7e2a9f1"
      },
      "source": [
        "#<a class=\"lemon--a__373c0__IEZFH link__373c0__29943 link-color--blue-dark__373c0__1mhJo link-size--inherit__373c0__2JXk5\" href=\"/biz/%E6%B7%BB%E5%A5%BD%E9%81%8B-%E9%A6%99%E6%B8%AF-6\" target=\"\" name=\"Tim Ho Wan\" rel=\"\">Tim Ho Wan</a>\n",
        "r = requests.get('https://www.yelp.com/search?cflt=restaurants&find_loc=Hong+Kong')\n",
        "bs = BeautifulSoup(r.text)\n",
        "\n",
        "result = bs.find_all('div',{'class':'lemon--div__373c0__1mboc businessNameWithNoVerifiedBadge__373c0__24q4s border-color--default__373c0__2oFDT'})\n",
        "#shopname= result.find_all('a',{'class':'lemon--a__373c0__IEZFH link__373c0__29943 link-color--blue-dark__373c0__1mhJo link-size--inherit__373c0__2JXk5\" href=\"/biz/%E6%B7%BB%E5%A5%BD%E9%81%8B-%E9%A6%99%E6%B8%AF-6'})\n",
        "type(result[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bs4.element.Tag"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywWD75NEkbRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2e95SMncbDU",
        "colab_type": "text"
      },
      "source": [
        "### Get total number of pages\n",
        "Write a function that return the toal pages of search results. Set a maximum number as the limit in the function.\n",
        "\n",
        "Example:\n",
        "- if total pages = 6 and max = 3, return 3\n",
        "- if total pages = 5 and max = 10, return 5\n",
        "- if no result, return -1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blG6vjUAkf6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_total_pages:\n",
        "  # code here\n",
        "  page 1 ->start=0\n",
        "  page 2 ->start=10\n",
        "  page k-> start =(k-1)*10\n",
        "  page 32->310\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YyO9Gx2kqcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use timer to make more humanlike, don let the web pade think you are bot\n",
        "def get_shop_list(url,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgWf4q8Ddm1z",
        "colab_type": "text"
      },
      "source": [
        "### Loop through pages and get a list of shop name and links\n",
        "Given a search result page, crawl all shop names and links for maximum 3 pages\n",
        "\n",
        "Sample output:\n",
        "<br>\n",
        "[['shop1','https://abc.com'],['shop2','https://xxxagd.com']]\n",
        "<br>\n",
        "* Remember to setup randomized random agent and timer to prevent bloclking by server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88H_9dxvu-Xv",
        "colab_type": "code",
        "outputId": "53a75961-9345-4ea3-f60e-db8ef27c70b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Install fake agent\n",
        "!pip install fake-useragent"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=399f3b2d5fcd06d7c0d31f6f049dc5c68ede0658683c1d8f2b14f3b122ab8870\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05X1b0CdvOsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fake_useragent import UserAgent\n",
        "import time\n",
        "import random\n",
        "ua = UserAgent()\n",
        "ua.random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttlRW5KXm7Td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get shop name and link\n",
        "def get_shop_list(url, max_page=3):\n",
        "    # code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssz8KBuafAdm",
        "colab_type": "text"
      },
      "source": [
        "### Get reviews\n",
        "Get reviews given a shop url, crawl maximum 3 pages of reviews\n",
        "\n",
        "sample output:\n",
        "<br>\n",
        "[['shop1', 3, 'it is just ok'], \n",
        "['shop1',5,'the food is delicious!'']]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL8lHP33m7Th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_reviews(shop_name, url, max_page=5):\n",
        "    # code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE6b-3mijnvD",
        "colab_type": "text"
      },
      "source": [
        "### Combine all together!\n",
        "Given a search query, return shop list and their reviews. \n",
        "\n",
        "\n",
        "Finally, export each review to a 3-columns csv files that contains \n",
        "1. shop name\n",
        "2. star of a review\n",
        "3. review content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaC8NiRmkvmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# crawl all review and return a list of list \n",
        "# code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af1RnFbWm7Ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}